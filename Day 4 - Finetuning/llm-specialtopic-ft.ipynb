{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Step 1: Install all the needed packages"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:14:01.705102Z","iopub.status.busy":"2024-08-02T16:14:01.704327Z","iopub.status.idle":"2024-08-02T16:14:01.710110Z","shell.execute_reply":"2024-08-02T16:14:01.709201Z","shell.execute_reply.started":"2024-08-02T16:14:01.705070Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-02T16:14:04.201096Z","iopub.status.busy":"2024-08-02T16:14:04.200718Z","iopub.status.idle":"2024-08-02T16:14:16.696763Z","shell.execute_reply":"2024-08-02T16:14:16.695704Z","shell.execute_reply.started":"2024-08-02T16:14:04.201066Z"},"trusted":true},"outputs":[],"source":["!pip install -q accelerate peft bitsandbytes transformers trl"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2: Import all the Required Libraries"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:14:43.114784Z","iopub.status.busy":"2024-08-02T16:14:43.113439Z","iopub.status.idle":"2024-08-02T16:14:43.120248Z","shell.execute_reply":"2024-08-02T16:14:43.119164Z","shell.execute_reply.started":"2024-08-02T16:14:43.114746Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    EarlyStoppingCallback\n",")\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["# Step 3: Get the model names and dataset names"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:14:45.659538Z","iopub.status.busy":"2024-08-02T16:14:45.659159Z","iopub.status.idle":"2024-08-02T16:14:45.664204Z","shell.execute_reply":"2024-08-02T16:14:45.663248Z","shell.execute_reply.started":"2024-08-02T16:14:45.659507Z"},"trusted":true},"outputs":[],"source":["model_name = 'NousResearch/Llama-2-7b-chat-hf'\n","dataset_name = 'mlabonne/guanaco-llama2'\n","new_model = 'Llama-2-7b-hf-chat-finetune'"]},{"cell_type":"markdown","metadata":{},"source":["# Step 4: Load the dataset and model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:14:47.818394Z","iopub.status.busy":"2024-08-02T16:14:47.818024Z","iopub.status.idle":"2024-08-02T16:14:52.850947Z","shell.execute_reply":"2024-08-02T16:14:52.850024Z","shell.execute_reply.started":"2024-08-02T16:14:47.818365Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab93e37897d74b6b896d7531fd31ad47","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/8.99M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"353282bbd9fb4a96b2d0a1b593e8831c","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/476k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2dc94a3ccc3457ba41e8626d026b062","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96671d41e3434fb997860d38e6fbc130","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset(dataset_name, split = 'train')\n","dataset = dataset.shuffle(seed=42).select(range(200))\n","\n","train_test_split = dataset.train_test_split(test_size=50)\n","train_dataset = train_test_split['train']\n","eval_dataset = train_test_split['test']"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:14:57.715304Z","iopub.status.busy":"2024-08-02T16:14:57.714945Z","iopub.status.idle":"2024-08-02T16:14:57.724488Z","shell.execute_reply":"2024-08-02T16:14:57.723389Z","shell.execute_reply.started":"2024-08-02T16:14:57.715276Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["150 50 {'text': '<s>[INST] Расскажи про атаку на Бисмарк [/INST] Атака на Бисмарк – это событие Второй мировой войны, произошедшее в мае 1941 года, когда британский флот атаковал немецкий линкор \"Бисмарк\" в Атлантике.\\n\\nЛинкор \"Бисмарк\" был одним из самых мощных и опасных кораблей своего времени. В мае 1941 года \"Бисмарк\" вместе с другим линкором \"Принц Евгений\" пытались прорваться в Атлантику, чтобы прервать снабжение союзников Германии, а также атаковать союзные корабли.\\n\\nБританский флот, получив информацию о движении \"Бисмарка\", решил пресечь его попытки прорваться в Атлантику. Были отправлены две эскадры – первая, в составе которой были линкоры \"Худ\" и \"Прицесс Роял\", а также несколько крейсеров, а вторая, более крупная, которая включала в себя боевые корабли \"Викториус\", \"Родней\", \"Норфолк\" и \"Суффолк\".\\n\\nПосле нескольких дней погони, \"Бисмарк\" был поражен торпедой британского самолета \"Скорпион\" и терпел значительные повреждения. Несмотря на это, он продолжал борьбу, но в конечном итоге был остановлен британскими кораблями и затоплен. В результате этой операции был уничтожен один из самых мощных кораблей Второй мировой войны.\\n\\nАтака на Бисмарк стала одной из крупнейших морских операций Второй мировой войны и считается одним из главных успехов британского флота. Она показала, что даже самые мощные корабли не могут выжить без поддержки союзников и что британский флот был способен нанести удар в любом месте мирового океана. </s>\\\\\\n<s>[INST] А что случилось с Гинденбургом? [/INST] По-видимому, вы имеете в виду крупнейший воздушный лайнер своего времени, немецкий дирижабль \"Гинденбург\" (LZ 129), который в результате катастрофы в Нью-Джерси, США, 6 мая 1937 года был уничтожен огнем. Катастрофа произошла во время приземления, когда лайнер внезапно загорелся и разрушился за несколько секунд, погибли 36 человек. Причина пожара до сих пор остается неясной, хотя были предложены различные теории, включая возможность неправильной обработки гелия, взрыв нефтяных паров или даже акт саботажа. Катастрофа \"Гинденбурга\" стала одним из самых трагических событий в истории авиации и стала причиной окончательного упадка популярности дирижаблей как средства коммерческого пассажирского транспорта. </s>'}\n"]}],"source":["print(len(train_dataset), len(eval_dataset), train_dataset[0])"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:15:00.282777Z","iopub.status.busy":"2024-08-02T16:15:00.282369Z","iopub.status.idle":"2024-08-02T16:15:00.289588Z","shell.execute_reply":"2024-08-02T16:15:00.288352Z","shell.execute_reply.started":"2024-08-02T16:15:00.282746Z"},"trusted":true},"outputs":[],"source":["bnb_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant = True\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:15:02.239115Z","iopub.status.busy":"2024-08-02T16:15:02.238699Z","iopub.status.idle":"2024-08-02T16:16:22.607903Z","shell.execute_reply":"2024-08-02T16:16:22.607083Z","shell.execute_reply.started":"2024-08-02T16:15:02.239081Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b3d9c8bf26f40c18d85ee9c446442c2","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea9150b3b1894a91bd7116297eedb979","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f407bcea59e4be4b6b795d99448eac6","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4dd552101be4fa78eae111a19d991c8","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5aed6b8f1f1b410eb33455fca890a262","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"979fd8c17f8f4a87a2d41f41946fb4d2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f68a4c912b84e88bc012942e2c89361","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de881546420344428d261bfaabb309c9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58bccca19acc4187bd5724233021225a","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96bbf5ec12484552afeeb7151fd95443","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bb4e3251f9e42f88ec596f1f239a6e9","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68cf1a7e718b474e9c783c62f719a416","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# loading the base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config = bnb_config,\n","    device_map = 'auto',\n","    token = ''\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1    # more accurate but slower computation\n","\n","# Loading LLaMa tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token='key_here')\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = 'right'    # Fix weird overflow issue with fp16 training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:16:23.263395Z","iopub.status.busy":"2024-08-02T16:16:23.263129Z","iopub.status.idle":"2024-08-02T16:16:26.086638Z","shell.execute_reply":"2024-08-02T16:16:26.085488Z","shell.execute_reply.started":"2024-08-02T16:16:23.263373Z"},"trusted":true},"outputs":[],"source":["# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha = 16,\n","    lora_dropout = 0.1,\n","    r = 64,\n","    bias = 'none',\n","    task_type = \"CAUSAL_LM\")\n","\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model,peft_config)"]},{"cell_type":"markdown","metadata":{},"source":["# Step 5: Training"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:16:29.553566Z","iopub.status.busy":"2024-08-02T16:16:29.553179Z","iopub.status.idle":"2024-08-02T16:16:29.590729Z","shell.execute_reply":"2024-08-02T16:16:29.589816Z","shell.execute_reply.started":"2024-08-02T16:16:29.553535Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["training_arguments = TrainingArguments(\n","    output_dir = '/kaggle/working',\n","    per_device_train_batch_size = 2,\n","    per_device_eval_batch_size = 2,\n","    gradient_checkpointing = True,\n","    gradient_accumulation_steps = 1,\n","    optim = 'paged_adamw_8bit',\n","    save_steps = 25,\n","    save_strategy = 'steps',\n","    evaluation_strategy = 'steps',\n","    eval_steps = 25,\n","    load_best_model_at_end = True,\n","    learning_rate = 2e-4,\n","    weight_decay = 0.001,\n","    fp16 = False,\n","    bf16 = False,\n","    max_grad_norm = 0.3,\n","    max_steps = -1,\n","    warmup_ratio = 0.03,\n","    group_by_length = True,\n","    lr_scheduler_type = 'cosine',\n","    report_to = 'tensorboard'\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:16:38.086260Z","iopub.status.busy":"2024-08-02T16:16:38.085888Z","iopub.status.idle":"2024-08-02T16:16:38.090684Z","shell.execute_reply":"2024-08-02T16:16:38.089666Z","shell.execute_reply.started":"2024-08-02T16:16:38.086232Z"},"trusted":true},"outputs":[],"source":["early_stopping = EarlyStoppingCallback(\n","    early_stopping_patience = 2,\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:18:24.005222Z","iopub.status.busy":"2024-08-02T16:18:24.004519Z","iopub.status.idle":"2024-08-02T16:18:24.265695Z","shell.execute_reply":"2024-08-02T16:18:24.264951Z","shell.execute_reply.started":"2024-08-02T16:18:24.005190Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81b9cdeb113043688bf2984668ffdd7f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/150 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c47252fc8cf4b02a8d520812cc85639","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/50 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model = model, \n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset,\n","    dataset_text_field = 'text',\n","    max_seq_length = None,\n","    tokenizer = tokenizer,\n","    args = training_arguments,\n","    callbacks = [early_stopping]\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T16:18:30.727139Z","iopub.status.busy":"2024-08-02T16:18:30.726761Z","iopub.status.idle":"2024-08-02T17:21:16.628426Z","shell.execute_reply":"2024-08-02T17:21:16.627182Z","shell.execute_reply.started":"2024-08-02T16:18:30.727110Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='200' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [200/225 1:01:58 < 07:49, 0.05 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>No log</td>\n","      <td>1.428499</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>No log</td>\n","      <td>1.281319</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>No log</td>\n","      <td>1.245929</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>No log</td>\n","      <td>1.235654</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>No log</td>\n","      <td>1.231044</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>No log</td>\n","      <td>1.230033</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>No log</td>\n","      <td>1.231645</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>No log</td>\n","      <td>1.232338</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=200, training_loss=1.3957742309570313, metrics={'train_runtime': 3765.2712, 'train_samples_per_second': 0.12, 'train_steps_per_second': 0.06, 'total_flos': 7355697750294528.0, 'train_loss': 1.3957742309570313, 'epoch': 2.6666666666666665})"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 6: Testing"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T17:21:16.631655Z","iopub.status.busy":"2024-08-02T17:21:16.630360Z","iopub.status.idle":"2024-08-02T17:21:18.049450Z","shell.execute_reply":"2024-08-02T17:21:18.048536Z","shell.execute_reply.started":"2024-08-02T17:21:16.631626Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt: Why did the chicken cross the road?\n","Model's response: Why did the chicken cross the road? To get to the other side!\n"]}],"source":["def generate_response(prompt, model, tokenizer, max_length=200):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    outputs = model.generate(**inputs, max_new_tokens=max_length, temperature=0.7, top_p=0.9)\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","# Example usage\n","prompt = \"Why did the chicken cross the road?\"\n","response = generate_response(prompt, model, tokenizer)\n","print(f\"Prompt: {prompt}\")\n","print(f\"Model's response: {response}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
